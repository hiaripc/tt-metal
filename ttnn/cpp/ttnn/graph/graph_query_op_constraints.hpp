// SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#pragma once

#include <optional>
#include <string>

#include <nlohmann/json.hpp>
#include "ttnn/graph/graph_processor.hpp"
#include "ttnn/graph/graph_trace_utils.hpp"

namespace ttnn::graph {

enum class ExecutionStatus { Success, Error };

struct ResourceUsage {
    size_t cb_peak_size_per_core = 0;
    size_t l1_buffers_peak_per_core = 0;
    size_t l1_output_buffer_per_core = 0;
};

struct QueryResponse {
    ExecutionStatus status = ExecutionStatus::Error;
    ResourceUsage resource_usage;
    std::optional<std::string> error_message;
};

/**
 * @brief Captures the graph operations and extracts resource usage constraints.
 *
 * This function runs graph capture by invoking the provided operation with the given arguments,
 * then extracts and returns the resource usage constraints from the captured trace.
 *
 * @tparam Op The type of the operation that will be invoked to capture the graph operations.
 * @tparam Args The types of the arguments that will be passed to the operation.
 * @param op The operation that will be invoked to capture the graph operations.
 * @param device A pointer to the Device object, which provides information about the compute grid size.
 * @param args The arguments that will be passed to the operation.
 * @return QueryResponse containing the execution status and resource usage constraints.
 *         - On success: ExecutionStatus::Success and the resource usage details.
 *         - On failure: ExecutionStatus::Error, zeroed resource usage, and an error message.
 */
template <typename Op, typename... Args>
auto query_op_constraints(Op op, Device* device, Args&&... args) {
    uint32_t num_of_active_graph_captures = 0;
    try {
        // outer graph capture is to avoid dispatching/allocating dummy input tensors
        GraphProcessor::begin_graph_capture(GraphProcessor::RunMode::NO_DISPATCH);
        num_of_active_graph_captures++;

        // helper lambda to transform TensorSpec to DeviceTensor
        auto transform_arg = [device](auto&& arg) {
            if constexpr (std::is_same_v<std::decay_t<decltype(arg)>, TensorSpec>) {
                return create_device_tensor(arg, device);
            } else {
                return std::forward<decltype(arg)>(arg);
            }
        };
        auto transformed_args = std::make_tuple(transform_arg(std::forward<Args>(args))...);

        // inner graph capture is to capture the actual op graph trace
        GraphProcessor::begin_graph_capture(GraphProcessor::RunMode::NO_DISPATCH);
        num_of_active_graph_captures++;
        std::apply(op, transformed_args);
        const nlohmann::json op_trace = GraphProcessor::end_graph_capture();  // end of inner graph capture
        num_of_active_graph_captures--;
        GraphProcessor::end_graph_capture();  // end of outer graph capture
        num_of_active_graph_captures--;

        // extract memory footprint from the trace
        auto interleaved_storage_cores = device->num_banks(tt::tt_metal::BufferType::L1);
        size_t cb_peak_size_per_core = extract_circular_buffers_peak_size_per_core(op_trace);
        size_t l1_buffers_peak_per_core =
            extract_l1_buffer_allocation_peak_size_per_core(op_trace, interleaved_storage_cores);
        size_t l1_output_buffer_per_core =
            extract_l1_output_buffer_allocation_size_per_core(op_trace, interleaved_storage_cores);

        return QueryResponse{
            ExecutionStatus::Success, {cb_peak_size_per_core, l1_buffers_peak_per_core, l1_output_buffer_per_core}};

    } catch (const std::exception& e) {
        // end all active graph captures
        for (uint32_t i = 0; i < num_of_active_graph_captures; i++) {
            GraphProcessor::end_graph_capture();
        }
        tt::log_debug(tt::LogOp, "op_constraints - error: {}", e.what());
        return QueryResponse{ExecutionStatus::Error, {0, 0, 0}, e.what()};
    }
}

}  // namespace ttnn::graph
